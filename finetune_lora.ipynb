{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0",
   "source": [
    "# SAR2Optical Fine-tuning with LoRA Adapters\n",
    "\n",
    "This notebook uses **LoRA (Low-Rank Adaptation)** to fine-tune the pretrained SAR2Optical model on QXSLAB_SAROPT.\n",
    "\n",
    "**Why LoRA?**\n",
    "- Freezes pretrained weights (preserves learned features)\n",
    "- Only trains small adapter layers (~1-5% of parameters)\n",
    "- Prevents catastrophic forgetting\n",
    "- Faster training, less overfitting\n",
    "- Can merge adapters back into original model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-1",
   "source": [
    "## 1. Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-2",
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-3",
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q gdown tqdm pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-4",
   "source": [
    "## 2. Download Dataset and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-5",
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "DATASET_FILE_ID = \"1835G9HBouBqmk7tKNnIc5gkJ5B8-4v9I\"  # <-- Your QXSLAB_SAROPT.zip ID\n",
    "!gdown {DATASET_FILE_ID} -O /content/QXSLAB_SAROPT.zip\n",
    "!unzip -q /content/QXSLAB_SAROPT.zip -d /content/\n",
    "!ls -la /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-6",
   "outputs": [],
   "source": [
    "# Download pretrained checkpoint\n",
    "CHECKPOINT_FILE_ID = \"1avb5ua7fYlgQOarS4Xvi3zpsX6s9Z7NV\"  # <-- Your checkpoint ID\n",
    "!gdown {CHECKPOINT_FILE_ID} -O /content/pix2pix_gen_180.pth\n",
    "!ls -lh /content/pix2pix_gen_180.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-7",
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-8",
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Paths\n",
    "DATASET_ROOT = \"/content/QXSLAB_SAROPT\"\n",
    "SAR_FOLDER = \"sar_256_oc_0.2\"\n",
    "OPT_FOLDER = \"opt_256_oc_0.2\"\n",
    "PRETRAINED_CHECKPOINT = \"/content/pix2pix_gen_180.pth\"\n",
    "OUTPUT_DIR = \"/content/lora_checkpoints\"\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_CONFIG = {\n",
    "    \"rank\": 16,              # LoRA rank (4-64, higher = more capacity)\n",
    "    \"alpha\": 32,             # LoRA alpha (scaling factor, usually 2x rank)\n",
    "    \"dropout\": 0.1,          # Dropout in LoRA layers\n",
    "    \"target_modules\": [\"encoder\", \"decoder\"],  # Where to add LoRA\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"c_in\": 3,\n",
    "    \"c_out\": 3,\n",
    "    \"lambda_L1\": 100.0,\n",
    "    \"use_upsampling\": False,\n",
    "    \"mode\": \"nearest\",\n",
    "    \n",
    "    # Training\n",
    "    \"num_epochs\": 30,         # Can train longer with LoRA\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.0002,             # Higher LR is OK for LoRA\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"save_freq\": 5,\n",
    "    \"num_workers\": 4,\n",
    "    \n",
    "    # Dataset split\n",
    "    \"split_ratio\": [0.8, 0.1, 0.1],\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank: {LORA_CONFIG['rank']}\")\n",
    "print(f\"  Alpha: {LORA_CONFIG['alpha']}\")\n",
    "print(f\"  Dropout: {LORA_CONFIG['dropout']}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-9",
   "source": [
    "## 4. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-10",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-11",
   "source": [
    "## 5. LoRA Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-12",
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LoRA LAYER IMPLEMENTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) layer for Conv2d.\n",
    "    \n",
    "    Instead of updating W directly, we learn:\n",
    "        W' = W + (alpha/rank) * B @ A\n",
    "    \n",
    "    Where A and B are low-rank matrices.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 1.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Low-rank matrices (A: down-projection, B: up-projection)\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialize A with Kaiming, B with zeros (so LoRA starts as identity)\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch, in_features) or will be reshaped\n",
    "        # Returns delta to add to original output\n",
    "        return (self.dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "\n",
    "\n",
    "class LoRAConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA adapter for Conv2d layers.\n",
    "    Wraps an existing Conv2d and adds low-rank adaptation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv: nn.Conv2d,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 1.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = conv\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Freeze original conv weights\n",
    "        for param in self.conv.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        in_channels = conv.in_channels\n",
    "        out_channels = conv.out_channels\n",
    "        kernel_size = conv.kernel_size[0]\n",
    "        \n",
    "        # LoRA uses 1x1 convolutions for efficiency\n",
    "        # A: (rank, in_channels, 1, 1) - down projection\n",
    "        # B: (out_channels, rank, 1, 1) - up projection\n",
    "        self.lora_A = nn.Conv2d(in_channels, rank, kernel_size=1, bias=False)\n",
    "        self.lora_B = nn.Conv2d(rank, out_channels, kernel_size=1, bias=False)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original conv output\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        # LoRA path: x -> A -> B -> scale\n",
    "        # Need to handle spatial dimensions\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        \n",
    "        # Match spatial size if different (due to stride/padding)\n",
    "        if lora_out.shape[2:] != out.shape[2:]:\n",
    "            lora_out = F.interpolate(lora_out, size=out.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return out + lora_out\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"Merge LoRA weights into original conv for inference.\"\"\"\n",
    "        # For 1x1 LoRA on larger kernels, we add to center\n",
    "        # This is a simplified merge - works best when kernel_size matches\n",
    "        with torch.no_grad():\n",
    "            # Get LoRA contribution as a 1x1 kernel\n",
    "            # lora_A: (rank, in_ch, 1, 1), lora_B: (out_ch, rank, 1, 1)\n",
    "            # Combined: (out_ch, in_ch, 1, 1)\n",
    "            lora_weight = (self.lora_B.weight @ self.lora_A.weight.view(self.rank, -1)).view(\n",
    "                self.conv.out_channels, self.conv.in_channels, 1, 1\n",
    "            ) * self.scaling\n",
    "            \n",
    "            # Add to center of original kernel\n",
    "            k = self.conv.kernel_size[0]\n",
    "            center = k // 2\n",
    "            self.conv.weight[:, :, center:center+1, center:center+1] += lora_weight\n",
    "        \n",
    "        return self.conv\n",
    "\n",
    "\n",
    "print(\"LoRA layers defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-13",
   "source": [
    "## 6. Base Network Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-14",
   "outputs": [],
   "source": "# ============================================================================\n# BASE NETWORK LAYERS (Modified for LoRA compatibility)\n# ============================================================================\n\nclass DownsamplingBlock(nn.Module):\n    def __init__(self, c_in, c_out, kernel_size=4, stride=2, \n                 padding=1, negative_slope=0.2, use_norm=True):\n        super().__init__()\n        # Keep conv as separate attribute so LoRA can replace it\n        self.conv = nn.Conv2d(c_in, c_out, kernel_size, stride, padding, bias=(not use_norm))\n        self.norm = nn.BatchNorm2d(c_out) if use_norm else nn.Identity()\n        self.act = nn.LeakyReLU(negative_slope)\n        \n    def forward(self, x):\n        return self.act(self.norm(self.conv(x)))\n\n\nclass UpsamplingBlock(nn.Module):\n    def __init__(self, c_in, c_out, kernel_size=4, stride=2, \n                 padding=1, use_dropout=False, use_upsampling=False, mode='nearest'):\n        super().__init__()\n        if use_upsampling:\n            self.conv = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=mode),\n                nn.Conv2d(c_in, c_out, 3, 1, padding, bias=False)\n            )\n        else:\n            self.conv = nn.ConvTranspose2d(c_in, c_out, kernel_size, stride, padding, bias=False)\n        self.norm = nn.BatchNorm2d(c_out)\n        self.dropout = nn.Dropout(0.5) if use_dropout else nn.Identity()\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        return self.act(self.dropout(self.norm(self.conv(x))))\n\n\nclass UnetEncoder(nn.Module):\n    def __init__(self, c_in=3):\n        super().__init__()\n        self.enc1 = DownsamplingBlock(c_in, 64, use_norm=False)\n        self.enc2 = DownsamplingBlock(64, 128)\n        self.enc3 = DownsamplingBlock(128, 256)\n        self.enc4 = DownsamplingBlock(256, 512)\n        self.enc5 = DownsamplingBlock(512, 512)\n        self.enc6 = DownsamplingBlock(512, 512)\n        self.enc7 = DownsamplingBlock(512, 512)\n        self.enc8 = DownsamplingBlock(512, 512)\n\n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        x4 = self.enc4(x3)\n        x5 = self.enc5(x4)\n        x6 = self.enc6(x5)\n        x7 = self.enc7(x6)\n        x8 = self.enc8(x7)\n        return [x8, x7, x6, x5, x4, x3, x2, x1]\n\n\nclass UnetDecoder(nn.Module):\n    def __init__(self, use_upsampling=False, mode='nearest'):\n        super().__init__()\n        self.dec1 = UpsamplingBlock(512, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode)\n        self.dec2 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode)\n        self.dec3 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode)\n        self.dec4 = UpsamplingBlock(1024, 512, use_upsampling=use_upsampling, mode=mode)\n        self.dec5 = UpsamplingBlock(1024, 256, use_upsampling=use_upsampling, mode=mode)\n        self.dec6 = UpsamplingBlock(512, 128, use_upsampling=use_upsampling, mode=mode)\n        self.dec7 = UpsamplingBlock(256, 64, use_upsampling=use_upsampling, mode=mode)\n        self.dec8 = UpsamplingBlock(128, 64, use_upsampling=use_upsampling, mode=mode)\n\n    def forward(self, x):\n        x9 = torch.cat([x[1], self.dec1(x[0])], 1)\n        x10 = torch.cat([x[2], self.dec2(x9)], 1)\n        x11 = torch.cat([x[3], self.dec3(x10)], 1)\n        x12 = torch.cat([x[4], self.dec4(x11)], 1)\n        x13 = torch.cat([x[5], self.dec5(x12)], 1)\n        x14 = torch.cat([x[6], self.dec6(x13)], 1)\n        x15 = torch.cat([x[7], self.dec7(x14)], 1)\n        return self.dec8(x15)\n\n\nclass UnetGenerator(nn.Module):\n    def __init__(self, c_in=3, c_out=3, use_upsampling=False, mode='nearest'):\n        super().__init__()\n        self.encoder = UnetEncoder(c_in=c_in)\n        self.decoder = UnetDecoder(use_upsampling=use_upsampling, mode=mode)\n        self.head = nn.Sequential(\n            nn.Conv2d(64, c_out, 3, 1, 1, bias=True),\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        return self.head(self.decoder(self.encoder(x)))\n\n\nprint(\"Base networks defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-15",
   "source": [
    "## 7. LoRA-enabled Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-16",
   "outputs": [],
   "source": "# ============================================================================\n# LoRA-ENABLED GENERATOR (SIMPLIFIED - Just wrap conv layers)\n# ============================================================================\n\nclass LoRAUnetGenerator(nn.Module):\n    \"\"\"\n    UNet Generator with LoRA adapters.\n    \n    SIMPLE APPROACH: Replace conv layers in-place with LoRA-wrapped versions.\n    This way we can use the original forward() method without changes.\n    \"\"\"\n    def __init__(\n        self, \n        base_generator: UnetGenerator,\n        rank: int = 16,\n        alpha: float = 32,\n        dropout: float = 0.1,\n        target_layers: List[str] = None,\n    ):\n        super().__init__()\n        self.base = base_generator\n        self.rank = rank\n        self.alpha = alpha\n        \n        # Freeze all base model parameters first\n        for param in self.base.parameters():\n            param.requires_grad = False\n        \n        # Default: add LoRA to encoder conv layers only (simpler, more stable)\n        if target_layers is None:\n            target_layers = ['enc']\n        \n        # Track LoRA modules for parameter collection\n        self.lora_modules = nn.ModuleList()\n        \n        # Add LoRA adapters by replacing conv layers in-place\n        self._add_lora_adapters(rank, alpha, dropout, target_layers)\n        \n        # Count parameters\n        total_params = sum(p.numel() for p in self.base.parameters())\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        print(f\"\\nTotal base parameters: {total_params:,}\")\n        print(f\"Trainable LoRA parameters: {trainable_params:,}\")\n        print(f\"Trainable ratio: {100*trainable_params/total_params:.2f}%\")\n    \n    def _add_lora_adapters(self, rank, alpha, dropout, target_layers):\n        \"\"\"Replace conv layers with LoRA-wrapped versions.\"\"\"\n        \n        # Process encoder\n        if 'enc' in target_layers or 'encoder' in target_layers:\n            for name in ['enc1', 'enc2', 'enc3', 'enc4', 'enc5', 'enc6', 'enc7', 'enc8']:\n                block = getattr(self.base.encoder, name)\n                # Create LoRA wrapper for the conv\n                lora_conv = LoRAConv2d(block.conv, rank=rank, alpha=alpha, dropout=dropout)\n                # Replace the conv in the block\n                block.conv = lora_conv\n                # Track for parameter collection\n                self.lora_modules.append(lora_conv)\n                print(f\"  Added LoRA to encoder.{name}: {lora_conv.conv.in_channels} -> {lora_conv.conv.out_channels}\")\n        \n        # Process decoder (optional - can be unstable)\n        if 'dec' in target_layers or 'decoder' in target_layers:\n            for name in ['dec1', 'dec2', 'dec3', 'dec4', 'dec5', 'dec6', 'dec7', 'dec8']:\n                block = getattr(self.base.decoder, name)\n                conv_layer = block.conv\n                \n                # Handle ConvTranspose2d\n                if isinstance(conv_layer, nn.ConvTranspose2d):\n                    lora_conv = LoRAConv2d(conv_layer, rank=rank, alpha=alpha, dropout=dropout)\n                    block.conv = lora_conv\n                    self.lora_modules.append(lora_conv)\n                    print(f\"  Added LoRA to decoder.{name}: {lora_conv.conv.in_channels} -> {lora_conv.conv.out_channels}\")\n                # Handle Sequential (upsampling mode)\n                elif isinstance(conv_layer, nn.Sequential):\n                    for i, layer in enumerate(conv_layer):\n                        if isinstance(layer, nn.Conv2d):\n                            lora_conv = LoRAConv2d(layer, rank=rank, alpha=alpha, dropout=dropout)\n                            conv_layer[i] = lora_conv\n                            self.lora_modules.append(lora_conv)\n                            print(f\"  Added LoRA to decoder.{name}[{i}]: {lora_conv.conv.in_channels} -> {lora_conv.conv.out_channels}\")\n                            break\n    \n    def forward(self, x):\n        # Simply use the base model's forward - LoRA is already embedded!\n        return self.base(x)\n    \n    def get_lora_parameters(self):\n        \"\"\"Return only LoRA parameters for optimizer.\"\"\"\n        params = []\n        for lora in self.lora_modules:\n            params.append(lora.lora_A.weight)\n            params.append(lora.lora_B.weight)\n        return params\n    \n    def save_lora_weights(self, path):\n        \"\"\"Save only LoRA weights (small file).\"\"\"\n        lora_state = {\n            'lora_modules': [\n                {\n                    'lora_A': lora.lora_A.state_dict(),\n                    'lora_B': lora.lora_B.state_dict(),\n                }\n                for lora in self.lora_modules\n            ],\n            'config': {'rank': self.rank, 'alpha': self.alpha}\n        }\n        torch.save(lora_state, path)\n        print(f\"LoRA weights saved to {path}\")\n    \n    def load_lora_weights(self, path):\n        \"\"\"Load LoRA weights.\"\"\"\n        lora_state = torch.load(path, map_location='cpu', weights_only=False)\n        for i, lora in enumerate(self.lora_modules):\n            lora.lora_A.load_state_dict(lora_state['lora_modules'][i]['lora_A'])\n            lora.lora_B.load_state_dict(lora_state['lora_modules'][i]['lora_B'])\n        print(f\"LoRA weights loaded from {path}\")\n\n\nprint(\"LoRA Generator defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-17",
   "source": [
    "## 8. Discriminator (unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-18",
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISCRIMINATOR (Standard PatchGAN)\n",
    "# ============================================================================\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, c_in=6, c_hid=64, n_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [DownsamplingBlock(c_in, c_hid, use_norm=False)]\n",
    "        \n",
    "        nf_mult = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            layers.append(DownsamplingBlock(c_hid * nf_mult_prev, c_hid * nf_mult))\n",
    "        \n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        layers.append(DownsamplingBlock(c_hid * nf_mult_prev, c_hid * nf_mult, stride=1))\n",
    "        layers.append(nn.Conv2d(c_hid * nf_mult, 1, kernel_size=4, stride=1, padding=1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "print(\"Discriminator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-19",
   "source": [
    "## 9. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-20",
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class QXSLABDataset(Dataset):\n",
    "    def __init__(self, root_dir, sar_folder=\"sar_256_oc_0.2\", opt_folder=\"opt_256_oc_0.2\",\n",
    "                 split=None, split_ratio=(0.8, 0.1, 0.1), augment=False, seed=42):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.sar_dir = self.root_dir / sar_folder\n",
    "        self.opt_dir = self.root_dir / opt_folder\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.Resize((256, 256)),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "        \n",
    "        # Find valid pairs\n",
    "        self.image_pairs = self._find_valid_pairs()\n",
    "        \n",
    "        if split:\n",
    "            self.image_pairs = self._apply_split(split, split_ratio, seed)\n",
    "            print(f\"[{split}] {len(self.image_pairs)} pairs (augment={augment})\")\n",
    "    \n",
    "    def _find_valid_pairs(self):\n",
    "        sar_files = {f.stem: f for f in self.sar_dir.glob(\"*.png\")}\n",
    "        opt_files = {f.stem: f for f in self.opt_dir.glob(\"*.png\")}\n",
    "        \n",
    "        pairs = [(sar_files[n], opt_files[n]) for n in sar_files if n in opt_files]\n",
    "        try:\n",
    "            pairs.sort(key=lambda x: int(x[0].stem))\n",
    "        except:\n",
    "            pairs.sort(key=lambda x: x[0].stem)\n",
    "        \n",
    "        print(f\"Found {len(pairs)} valid pairs\")\n",
    "        return pairs\n",
    "    \n",
    "    def _apply_split(self, split, split_ratio, seed):\n",
    "        random.seed(seed)\n",
    "        indices = list(range(len(self.image_pairs)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        n = len(indices)\n",
    "        train_end = int(n * split_ratio[0])\n",
    "        val_end = train_end + int(n * split_ratio[1])\n",
    "        \n",
    "        if split == 'train':\n",
    "            indices = indices[:train_end]\n",
    "        elif split == 'val':\n",
    "            indices = indices[train_end:val_end]\n",
    "        elif split == 'test':\n",
    "            indices = indices[val_end:]\n",
    "        \n",
    "        return [self.image_pairs[i] for i in indices]\n",
    "    \n",
    "    def _apply_augmentation(self, sar_img, opt_img):\n",
    "        if random.random() > 0.5:\n",
    "            sar_img = TF.hflip(sar_img)\n",
    "            opt_img = TF.hflip(opt_img)\n",
    "        if random.random() > 0.5:\n",
    "            sar_img = TF.vflip(sar_img)\n",
    "            opt_img = TF.vflip(opt_img)\n",
    "        angle = random.choice([0, 90, 180, 270])\n",
    "        if angle != 0:\n",
    "            sar_img = TF.rotate(sar_img, angle)\n",
    "            opt_img = TF.rotate(opt_img, angle)\n",
    "        return sar_img, opt_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sar_path, opt_path = self.image_pairs[idx]\n",
    "        sar_img = Image.open(sar_path).convert('RGB')\n",
    "        opt_img = Image.open(opt_path).convert('RGB')\n",
    "        \n",
    "        if self.augment:\n",
    "            sar_img, opt_img = self._apply_augmentation(sar_img, opt_img)\n",
    "        \n",
    "        return self.transform(sar_img), self.transform(opt_img)\n",
    "\n",
    "\n",
    "print(\"Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-21",
   "source": [
    "## 10. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-22",
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating datasets...\\n\")\n",
    "\n",
    "train_dataset = QXSLABDataset(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    split='train',\n",
    "    augment=True,\n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "val_dataset = QXSLABDataset(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    split='val',\n",
    "    augment=False,\n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                          shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
    "                        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-23",
   "source": [
    "## 11. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-24",
   "outputs": [],
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\\n\")\n\n# ============================================================================\n# LOAD PRETRAINED WEIGHTS WITH KEY REMAPPING\n# ============================================================================\n\ndef remap_state_dict(old_state_dict):\n    \"\"\"\n    Remap keys from old format (conv_block) to new format (conv, norm, act).\n    Old: encoder.enc1.conv_block.0.weight -> New: encoder.enc1.conv.weight\n    \"\"\"\n    new_state_dict = {}\n    \n    for key, value in old_state_dict.items():\n        new_key = key\n        \n        # Encoder remapping: conv_block.0 -> conv, conv_block.1 -> norm\n        if 'encoder' in key and 'conv_block' in key:\n            if '.conv_block.0.' in key:\n                new_key = key.replace('.conv_block.0.', '.conv.')\n            elif '.conv_block.1.' in key:\n                new_key = key.replace('.conv_block.1.', '.norm.')\n        \n        # Decoder remapping: conv_block.0 -> conv, conv_block.1 -> norm\n        if 'decoder' in key and 'conv_block' in key:\n            if '.conv_block.0.' in key:\n                new_key = key.replace('.conv_block.0.', '.conv.')\n            elif '.conv_block.1.' in key:\n                new_key = key.replace('.conv_block.1.', '.norm.')\n        \n        new_state_dict[new_key] = value\n    \n    return new_state_dict\n\n\n# Create base generator\nprint(\"Creating base generator...\")\nbase_gen = UnetGenerator(c_in=3, c_out=3, use_upsampling=CONFIG['use_upsampling'])\n\n# Load and remap pretrained weights\nprint(f\"Loading pretrained weights from: {PRETRAINED_CHECKPOINT}\")\nold_state_dict = torch.load(PRETRAINED_CHECKPOINT, map_location=device, weights_only=False)\nnew_state_dict = remap_state_dict(old_state_dict)\n\n# Load with strict=False to handle any remaining mismatches\nmissing, unexpected = base_gen.load_state_dict(new_state_dict, strict=False)\nif missing:\n    print(f\"  Missing keys: {len(missing)}\")\nif unexpected:\n    print(f\"  Unexpected keys: {len(unexpected)}\")\nprint(\"Pretrained weights loaded!\\n\")\n\n# Test base model first (must use eval mode for BatchNorm with small spatial sizes)\nprint(\"Testing base model...\")\ntest_input = torch.randn(1, 3, 256, 256).to(device)\nbase_gen.to(device)\nbase_gen.eval()  # IMPORTANT: eval mode for BatchNorm\nwith torch.no_grad():\n    test_output = base_gen(test_input)\nprint(f\"  Input: {test_input.shape} -> Output: {test_output.shape}\")\nprint(\"Base model works!\\n\")\n\n# Now wrap with LoRA\nprint(\"Adding LoRA adapters...\")\ngenerator = LoRAUnetGenerator(\n    base_gen,\n    rank=LORA_CONFIG['rank'],\n    alpha=LORA_CONFIG['alpha'],\n    dropout=LORA_CONFIG['dropout'],\n    target_layers=['enc'],  # Only encoder for stability\n).to(device)\n\n# Test LoRA model\nprint(\"\\nTesting LoRA model...\")\ngenerator.eval()  # eval mode for test\nwith torch.no_grad():\n    test_output_lora = generator(test_input)\nprint(f\"  Input: {test_input.shape} -> Output: {test_output_lora.shape}\")\nprint(\"LoRA model works!\\n\")\n\n# Create discriminator\nprint(\"Creating discriminator...\")\ndiscriminator = PatchDiscriminator(c_in=6).to(device)\ndisc_params = sum(p.numel() for p in discriminator.parameters())\nprint(f\"Discriminator parameters: {disc_params:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-25",
   "outputs": [],
   "source": [
    "# Optimizers - ONLY train LoRA params for generator\n",
    "lora_params = generator.get_lora_parameters()\n",
    "print(f\"LoRA parameters to train: {sum(p.numel() for p in lora_params):,}\")\n",
    "\n",
    "optimizer_G = torch.optim.Adam(lora_params, lr=CONFIG['lr'], betas=(CONFIG['beta1'], CONFIG['beta2']))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=CONFIG['lr'], betas=(CONFIG['beta1'], CONFIG['beta2']))\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "print(\"Optimizers created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-26",
   "source": [
    "## 12. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-27",
   "outputs": [],
   "source": [
    "def train_epoch(gen, disc, loader, opt_G, opt_D, device, epoch):\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    \n",
    "    total_D, total_G, total_L1 = 0, 0, 0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for sar, opt in pbar:\n",
    "        sar, opt = sar.to(device), opt.to(device)\n",
    "        batch_size = sar.size(0)\n",
    "        \n",
    "        # Generate fake\n",
    "        fake = gen(sar)\n",
    "        \n",
    "        # ----- Train Discriminator -----\n",
    "        opt_D.zero_grad()\n",
    "        \n",
    "        real_pair = torch.cat([sar, opt], 1)\n",
    "        fake_pair = torch.cat([sar, fake.detach()], 1)\n",
    "        \n",
    "        pred_real = disc(real_pair)\n",
    "        pred_fake = disc(fake_pair)\n",
    "        \n",
    "        loss_D = 0.5 * (\n",
    "            criterion_GAN(pred_real, torch.ones_like(pred_real)) +\n",
    "            criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "        )\n",
    "        loss_D.backward()\n",
    "        opt_D.step()\n",
    "        \n",
    "        # ----- Train Generator (LoRA only) -----\n",
    "        opt_G.zero_grad()\n",
    "        \n",
    "        fake_pair = torch.cat([sar, fake], 1)\n",
    "        pred_fake = disc(fake_pair)\n",
    "        \n",
    "        loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        loss_G_L1 = criterion_L1(fake, opt)\n",
    "        loss_G = loss_G_GAN + CONFIG['lambda_L1'] * loss_G_L1\n",
    "        \n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "        \n",
    "        total_D += loss_D.item()\n",
    "        total_G += loss_G.item()\n",
    "        total_L1 += loss_G_L1.item()\n",
    "        \n",
    "        pbar.set_postfix({'D': f\"{loss_D.item():.3f}\", 'G': f\"{loss_G.item():.3f}\", 'L1': f\"{loss_G_L1.item():.3f}\"})\n",
    "    \n",
    "    n = len(loader)\n",
    "    return {'D': total_D/n, 'G': total_G/n, 'L1': total_L1/n}\n",
    "\n",
    "\n",
    "def validate(gen, disc, loader, device):\n",
    "    gen.eval()\n",
    "    disc.eval()\n",
    "    \n",
    "    total_D, total_G, total_L1 = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sar, opt in loader:\n",
    "            sar, opt = sar.to(device), opt.to(device)\n",
    "            fake = gen(sar)\n",
    "            \n",
    "            real_pair = torch.cat([sar, opt], 1)\n",
    "            fake_pair = torch.cat([sar, fake], 1)\n",
    "            \n",
    "            pred_real = disc(real_pair)\n",
    "            pred_fake = disc(fake_pair)\n",
    "            \n",
    "            loss_D = 0.5 * (\n",
    "                criterion_GAN(pred_real, torch.ones_like(pred_real)) +\n",
    "                criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "            )\n",
    "            loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "            loss_G_L1 = criterion_L1(fake, opt)\n",
    "            loss_G = loss_G_GAN + CONFIG['lambda_L1'] * loss_G_L1\n",
    "            \n",
    "            total_D += loss_D.item()\n",
    "            total_G += loss_G.item()\n",
    "            total_L1 += loss_G_L1.item()\n",
    "    \n",
    "    n = len(loader)\n",
    "    return {'D': total_D/n, 'G': total_G/n, 'L1': total_L1/n}\n",
    "\n",
    "\n",
    "def visualize(gen, loader, device, num=4):\n",
    "    gen.eval()\n",
    "    sar, opt = next(iter(loader))\n",
    "    sar, opt = sar[:num].to(device), opt[:num].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake = gen(sar)\n",
    "    \n",
    "    sar = (sar * 0.5 + 0.5).cpu()\n",
    "    opt = (opt * 0.5 + 0.5).cpu()\n",
    "    fake = (fake * 0.5 + 0.5).clamp(0, 1).cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(num, 3, figsize=(12, 3*num))\n",
    "    for i in range(num):\n",
    "        axes[i,0].imshow(sar[i].permute(1,2,0))\n",
    "        axes[i,0].set_title('SAR Input')\n",
    "        axes[i,0].axis('off')\n",
    "        axes[i,1].imshow(fake[i].permute(1,2,0))\n",
    "        axes[i,1].set_title('Generated')\n",
    "        axes[i,1].axis('off')\n",
    "        axes[i,2].imshow(opt[i].permute(1,2,0))\n",
    "        axes[i,2].set_title('Ground Truth')\n",
    "        axes[i,2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-28",
   "source": [
    "## 13. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-29",
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "history = {'train_D':[], 'train_G':[], 'train_L1':[], 'val_D':[], 'val_G':[], 'val_L1':[]}\n",
    "best_val_l1 = float('inf')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING LoRA FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"LoRA Rank: {LORA_CONFIG['rank']}\")\n",
    "print(f\"LoRA Alpha: {LORA_CONFIG['alpha']}\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Learning rate: {CONFIG['lr']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-30",
   "outputs": [],
   "source": [
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    # Train\n",
    "    train_loss = train_epoch(generator, discriminator, train_loader, \n",
    "                             optimizer_G, optimizer_D, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(generator, discriminator, val_loader, device)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_D'].append(train_loss['D'])\n",
    "    history['train_G'].append(train_loss['G'])\n",
    "    history['train_L1'].append(train_loss['L1'])\n",
    "    history['val_D'].append(val_loss['D'])\n",
    "    history['val_G'].append(val_loss['G'])\n",
    "    history['val_L1'].append(val_loss['L1'])\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train[D:{train_loss['D']:.4f} G:{train_loss['G']:.4f} L1:{train_loss['L1']:.4f}] \"\n",
    "          f\"Val[D:{val_loss['D']:.4f} G:{val_loss['G']:.4f} L1:{val_loss['L1']:.4f}]\")\n",
    "    \n",
    "    # Save best model (based on val L1)\n",
    "    if val_loss['L1'] < best_val_l1:\n",
    "        best_val_l1 = val_loss['L1']\n",
    "        generator.save_lora_weights(f\"{OUTPUT_DIR}/lora_weights_best.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"{OUTPUT_DIR}/disc_best.pth\")\n",
    "        print(f\"  -> New best! Val L1: {best_val_l1:.4f}\")\n",
    "    \n",
    "    # Periodic checkpoint\n",
    "    if epoch % CONFIG['save_freq'] == 0:\n",
    "        generator.save_lora_weights(f\"{OUTPUT_DIR}/lora_weights_epoch{epoch}.pth\")\n",
    "        print(f\"  -> Checkpoint saved\")\n",
    "    \n",
    "    # Visualize\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        visualize(generator, val_loader, device)\n",
    "\n",
    "# Save final\n",
    "generator.save_lora_weights(f\"{OUTPUT_DIR}/lora_weights_final.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"Best Val L1: {best_val_l1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-31",
   "source": [
    "## 14. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-32",
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "epochs = range(1, len(history['train_D'])+1)\n",
    "\n",
    "axes[0].plot(epochs, history['train_D'], label='Train')\n",
    "axes[0].plot(epochs, history['val_D'], label='Val')\n",
    "axes[0].set_title('Discriminator Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(epochs, history['train_G'], label='Train')\n",
    "axes[1].plot(epochs, history['val_G'], label='Val')\n",
    "axes[1].set_title('Generator Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].plot(epochs, history['train_L1'], label='Train')\n",
    "axes[2].plot(epochs, history['val_L1'], label='Val')\n",
    "axes[2].set_title('L1 Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_history.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-33",
   "source": [
    "## 15. Load Best Model and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-34",
   "outputs": [],
   "source": [
    "# Load best LoRA weights\n",
    "generator.load_lora_weights(f\"{OUTPUT_DIR}/lora_weights_best.pth\")\n",
    "print(\"Best model loaded!\\n\")\n",
    "\n",
    "print(\"Final Results on Validation Set:\")\n",
    "visualize(generator, val_loader, device, num=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-35",
   "source": [
    "## 16. Export Merged Model (Optional)\n",
    "\n",
    "Merge LoRA weights into the base model for deployment (no LoRA overhead at inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-36",
   "outputs": [],
   "source": [
    "# Option 1: Save just LoRA weights (small file, ~2-5 MB)\n",
    "print(\"LoRA weights saved at:\")\n",
    "!ls -lh {OUTPUT_DIR}/lora_weights_*.pth\n",
    "\n",
    "# Option 2: Export merged model (full size, ~200 MB, no LoRA needed at inference)\n",
    "# Uncomment to use:\n",
    "# merged_model = generator.merge_and_export(f\"{OUTPUT_DIR}/merged_generator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-37",
   "source": [
    "## 17. Download Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-38",
   "outputs": [],
   "source": [
    "# List all saved files\n",
    "print(\"Saved files:\")\n",
    "!ls -lh {OUTPUT_DIR}/\n",
    "\n",
    "# Note: LoRA weights are very small (~2-5 MB) so easy to download!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-39",
   "outputs": [],
   "source": [
    "# Mount Google Drive to save\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy to Drive\n",
    "!cp -r {OUTPUT_DIR}/* /content/drive/MyDrive/SAR2Optical_LoRA/\n",
    "print(\"Files copied to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-40",
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What LoRA Does:\n",
    "- Freezes pretrained weights (54M parameters)\n",
    "- Only trains small adapter layers (~500K-2M parameters)\n",
    "- Prevents catastrophic forgetting of learned features\n",
    "- Faster training, less overfitting\n",
    "\n",
    "### Files Saved:\n",
    "- `lora_weights_best.pth` - Best LoRA weights (~2-5 MB)\n",
    "- `lora_weights_final.pth` - Final LoRA weights\n",
    "- `disc_best.pth` - Discriminator weights\n",
    "\n",
    "### For Inference:\n",
    "1. Load base generator with pretrained weights\n",
    "2. Wrap with LoRAUnetGenerator\n",
    "3. Load LoRA weights\n",
    "4. Run inference\n",
    "\n",
    "Or merge LoRA into base model for deployment without LoRA overhead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}